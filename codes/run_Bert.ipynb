{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import time\n",
    "import io\n",
    "import codecs\n",
    "\n",
    "import rougee\n",
    "from extractor import PacSumExtractorWithBert, PacSumExtractorWithTfIdf\n",
    "from data_iterator import Dataset\n",
    "import rougee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_dataset = Dataset('C:/WS 2019/Neural Text Summarization/project/data/CNN_DM/cd.validation.h5df')\n",
    "tune_dataset_iterator = tune_dataset.iterate_once_doc_bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = PacSumExtractorWithTBert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _select_tops(edge_scores, beta, lambda1, lambda2):\n",
    "\n",
    "        min_score = edge_scores.min()\n",
    "        max_score = edge_scores.max()\n",
    "        edge_threshold = min_score + beta * (max_score - min_score)\n",
    "        new_edge_scores = edge_scores - edge_threshold\n",
    "        forward_scores, backward_scores, _ = _compute_scores(new_edge_scores, 0)\n",
    "        forward_scores = 0 - forward_scores\n",
    "\n",
    "        paired_scores = []\n",
    "        for node in range(len(forward_scores)):\n",
    "            paired_scores.append([node,  lambda1 * forward_scores[node] + lambda2 * backward_scores[node]])\n",
    "\n",
    "        #shuffle to avoid any possible bias\n",
    "        random.shuffle(paired_scores)\n",
    "        paired_scores.sort(key = lambda x: x[1], reverse = True)\n",
    "        extracted = [item[0] for item in paired_scores[:3]]\n",
    "\n",
    "\n",
    "        return extracted\n",
    "\n",
    "def _compute_scores(similarity_matrix, edge_threshold):\n",
    "\n",
    "    forward_scores = [0 for i in range(len(similarity_matrix))]\n",
    "    backward_scores = [0 for i in range(len(similarity_matrix))]\n",
    "    edges = []\n",
    "    for i in range(len(similarity_matrix)):\n",
    "        for j in range(i+1, len(similarity_matrix[i])):\n",
    "            edge_score = similarity_matrix[i][j]\n",
    "            if edge_score > edge_threshold:\n",
    "                forward_scores[j] += edge_score\n",
    "                backward_scores[i] += edge_score\n",
    "                edges.append((i,j,edge_score))\n",
    "\n",
    "    return np.asarray(forward_scores), np.asarray(backward_scores), edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tune_extractor(edge_scores):\n",
    "\n",
    "        tops_list = []\n",
    "        hparam_list = []\n",
    "        num = 10\n",
    "        for k in range(num + 1):\n",
    "            beta = k / num\n",
    "            for i in range(11):\n",
    "                lambda1 = i/10\n",
    "                lambda2 = 1 - lambda1\n",
    "                extracted = _select_tops(edge_scores, beta=beta, lambda1=lambda1, lambda2=lambda2)\n",
    "\n",
    "                tops_list.append(extracted)\n",
    "                hparam_list.append((beta, lambda1, lambda2))\n",
    "\n",
    "        return tops_list, hparam_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_num=1000\n",
    "\n",
    "\n",
    "summaries, references = [], []\n",
    "k = 0\n",
    "for item in tune_dataset_iterator:\n",
    "    \n",
    "    article, abstract, inputs = item\n",
    "    edge_scores =extractor._calculate_similarity_matrix(*inputs)\n",
    "    tops_list, hparam_list = _tune_extractor(edge_scores)\n",
    "\n",
    "    summary_list = [list(map(lambda x: article[x], ids)) for ids in tops_list]\n",
    "    summaries.append(summary_list)\n",
    "    references.append([abstract])\n",
    "    k += 1\n",
    "    print(k)\n",
    "    if k % example_num == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "best_rouge = 0\n",
    "best_hparam = None\n",
    "for i in range(len(summaries[0])):\n",
    "    print(i)\n",
    "    print(\"threshold :  \"+str(hparam_list[i])+'\\n')\n",
    "    #print(\"non-lead ratio : \"+str(ratios[i])+'\\n')\n",
    "    for k in range(len(summaries)):\n",
    "        #print(i)\n",
    "        print(k)\n",
    "        summ = summaries[k][i]\n",
    "        #print('summ')\n",
    "        #print(len(summ))\n",
    "        #print(summaries)\n",
    "        ref = references[k][0]\n",
    "        \n",
    "        if len(summ) == len(ref):\n",
    "            count = count + 1\n",
    "            \n",
    "            print('sum')\n",
    "            print(len(summ))\n",
    "            if i != 8:\n",
    "                if summaries[k][i][0]!='.' and summaries[k][i][1]!='.' and summaries[k][i][2]!='.':\n",
    "                    result = rouge.get_scores(summ, ref, avg = True)\n",
    "                    #print(result)\n",
    "                    if result['rouge-l']['f'] > best_rouge:\n",
    "                        best_rouge = result['rouge-l']['f']\n",
    "                        best_hparam = hparam_list[i]\n",
    "\n",
    "print(\"The best hyper-parameter :  beta %.4f , lambda1 %.4f, lambda2 %.4f \" % (best_hparam[0], best_hparam[1], best_hparam[2]))\n",
    "print(\"The best rouge_1_f_score :  %.4f \" % best_rouge)\n",
    "\n",
    "beta = best_hparam[0]\n",
    "lambda1 = best_hparam[1]\n",
    "lambda2 = best_hparam[2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
